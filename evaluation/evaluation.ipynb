{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d21c83-4caa-4094-a25a-fc3aab67290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from json.decoder import JSONDecodeError\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import nest_asyncio\n",
    "import polars as pl\n",
    "import asyncio\n",
    "import shelve\n",
    "import json\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495e0ca-0cfe-45e5-a97c-44fec1d26c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24215b-6e1b-4144-8050-474435f5e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e3114-1075-4e10-861b-235a587d5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../core'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d62b38-5cde-43d0-8c39-cfd330d5d91f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from retrival import VectorSearcher, HybridSearcher\n",
    "from generation import LLM\n",
    "from rag import ChatGourmet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c32d43-3859-4fb5-a2d3-830cff2d2ac6",
   "metadata": {},
   "source": [
    "## Get questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d4d005-b05c-4411-aa9e-19c086c72575",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = pl.read_csv(\"./dataset/synthetic-questions.csv\").to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1341bbd-c8e3-40ec-bd2f-df69b3027d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f1f924-8f00-4266-b85e-022e503f87ac",
   "metadata": {},
   "source": [
    "# Generate answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e173a-f3c0-4e12-a265-4feb5486d314",
   "metadata": {},
   "source": [
    "## Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f6742-7f78-4a84-b416-959b58ca9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(client, question_list, q_rewrite_func=False, attempt=1, max_attempts=5):\n",
    "    # The actual query rewrite function is not set to check twice by default as in the rag we will be iterating in a chat and not necessarily will need a search, can be some comments\n",
    "    answers = []\n",
    "    no_answers = []\n",
    "\n",
    "    for question in tqdm(question_list):\n",
    "        qid, q = list(question.values())[:2]\n",
    "        final_q = q\n",
    "        \n",
    "        if callable(q_rewrite_func):\n",
    "            q_decision = q_rewrite_func(q)\n",
    "            if q_decision[\"search\"] == \"yes\":\n",
    "                final_q = q_decision[\"query\"]\n",
    "            elif q_decision[\"search\"] == \"no\":\n",
    "                no_answers.append({'question_id': qid, 'question': q, \"rank\": 99,\"answer\": \"NO QUERY\"})\n",
    "                continue\n",
    "        \n",
    "        results = client.search(final_q)\n",
    "        results = list(enumerate(results, start=1))\n",
    "        \n",
    "        if not results:\n",
    "            no_answers.append({'question_id': qid, 'question': q, \"rank\": 98,\"answer\": \"NO ANSWER\"})\n",
    "            continue\n",
    "        \n",
    "        for r, a in results:\n",
    "            answers.append({'question_id': qid, 'question': q, \"rank\": r,\"answer\": a})\n",
    "\n",
    "    # When using query rewrite the LLM can sometime at first reply as seach as no even it should be yes\n",
    "    if callable(q_rewrite_func) and attempt <= max_attempts:\n",
    "        print(f\"No answers: {len(no_answers)} | Attempt num: {attempt}\")\n",
    "        attempt_answers, final_no_answers = generate_answers(client, no_answers, q_rewrite_func, attempt=attempt+1, max_attempts=max_attempts)\n",
    "        answers += attempt_answers\n",
    "        no_answers = final_no_answers\n",
    "    \n",
    "    return answers, no_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834218ff-adf5-418b-9ecf-7d88987e780a",
   "metadata": {},
   "source": [
    "### VectorSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1b94d-65e8-4e14-a7d4-ee211a6a2e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_client = VectorSearcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80a64d-727c-4a10-9d2c-45e7fac653b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_answers, vector_no_answers = generate_answers(vector_client, question_list)\n",
    "\n",
    "print(len(vector_no_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177d5cc-6ede-4734-86bb-4e4f2bd64179",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_dicts(vector_answers).write_csv(\"./dataset/vector-answers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89024f0-a872-4a15-bc2f-2f990fba3b1a",
   "metadata": {},
   "source": [
    "### HybridSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc867f53-6aee-451b-8f57-37508d63eb01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hybrid_client = HybridSearcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af46e1ba-86c9-44e4-9cb2-2e43644e4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_answers, hybrid_no_answers = generate_answers(hybrid_client, question_list)\n",
    "\n",
    "print(len(hybrid_no_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914a38d-2759-474a-b913-99a78ff15291",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_dicts(hybrid_answers).write_csv(\"./dataset/hybrid-answers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4099b-8ae1-4aa4-a1b8-ac41b76a88a2",
   "metadata": {},
   "source": [
    "### Query rewrite + HybridSearcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a9ce4-c763-41f3-a2ab-d94104a6a2f4",
   "metadata": {},
   "source": [
    "# NEED TO RERUN FOR NEW CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d4041-62c4-41cc-ab01-c66950bb4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = ChatGourmet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df012c-37f2-4d2d-9deb-11c8dc947cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rewrite(q):\n",
    "    return asyncio.run(cg._query_rewrite(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e8e10-7908-4f1f-82b6-ee449ef4e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_list[0]['question'])\n",
    "print(query_rewrite(question_list[0]['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf309d2-c0a6-4ae4-b724-ae23611ac4e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qrewrite_answers, qrewrite_no_answers = generate_answers(vector_client, question_list, query_rewrite, attempt=1, max_attempts=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aae7b7-2fe6-4aef-ba52-7f9e2c8a7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qrewrite_no_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6ee96-6d57-45f1-97e7-42fce5c8e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_dicts(qrewrite_answers).write_csv(\"./dataset/qrewrite-answers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c78dd-6a80-4378-a037-f1ded91c1a0b",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036342a-4a1f-48e9-8965-59146cfe8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rag:\n",
    "    def __init__(self):\n",
    "        self._client = OpenAI(base_url=f\"http://localhost:8000/v1\", api_key=None)\n",
    "    \n",
    "    def chat(self, question):\n",
    "        try:\n",
    "            q = question['question']\n",
    "            response = self._client.chat.completions.create(\n",
    "                model=None,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": q}\n",
    "                ]\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            question['answer'] = content\n",
    "            return question\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "rag = Rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d87c9-c19e-4b02-8b05-e1191bb92c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.chat({\"question\":\"How do I make a lemon herb baked salmon?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f6d3d-219d-4eb4-858c-8ffb01a4f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_answers(question_list, db_name, num_threads=4, batch_size=50, attempt=1, max_attempts=3):\n",
    "    counter = 0\n",
    "    \n",
    "    with shelve.open(f'./shelve/{db_name}') as db:\n",
    "        if 'remaining' not in db:\n",
    "            db['processed'] = []\n",
    "            db['remaining'] = question_list\n",
    "\n",
    "        processed_question = db['processed']\n",
    "        remaining_question = db['remaining']\n",
    "\n",
    "    remaining_question_dict = {str(ans['question_id']): ans for ans in remaining_question}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(rag.chat, question) for question in remaining_question]\n",
    "\n",
    "        for future in tqdm(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                processed_question.append(result)\n",
    "                remaining_question_dict.pop(str(result['question_id']), None)\n",
    "\n",
    "                counter += 1\n",
    "                if counter % batch_size == 0:\n",
    "                    remaining_answers = list(remaining_answers_dict.values())\n",
    "                    with shelve.open(f'./shelve/{db_name}') as db:\n",
    "                        db['processed'] = processed_question\n",
    "                        db['remaining'] = remaining_question\n",
    "\n",
    "    remaining_question = list(remaining_question_dict.values())\n",
    "    with shelve.open(f'./shelve/{db_name}') as db:\n",
    "        db['processed'] = processed_question\n",
    "        db['remaining'] = remaining_question\n",
    "\n",
    "    if remaining_question and attempt <= max_attempts:\n",
    "        print(f\"Remaining questions: {len(remaining_question)} | Attempt num: {attempt}\")\n",
    "        generate_rag_answers(remaining_question, db_name, num_threads=num_threads, attempt=attempt+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a884b-4cab-4e05-85c7-2bd3025fab8e",
   "metadata": {},
   "source": [
    "### RAG - meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "The model is selected based on what is running on `vllm-serve` in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0231198-bede-46b0-9c29-e9f78affc76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_rag_answers(question_list, \"Llama-3.1-8B-answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004e07d-3206-4e12-9ee2-1c74b7ad9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with shelve.open('./shelve/Llama-3.1-8B-answer') as db:\n",
    "    llama_31_8b_answers = db['processed']\n",
    "    llama_31_8b_no_answers  = db['remaining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c43f8-8053-46a7-b240-fe47a2c42f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_dicts(llama_31_8b_answers).write_csv(\"./dataset/Llama-3.1-8B-answer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229be4f4-6642-40ba-90d7-49d4d3a954e4",
   "metadata": {},
   "source": [
    "### RAG - microsoft/Phi-3.5-mini-instruct\n",
    "\n",
    "The model is selected based on what is running on `vllm-serve` in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c06e9-c4eb-4451-99c9-03d6a71dad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_rag_answers(question_list, \"Phi-3.5-mini-answer\", batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a1b47-12a8-47d7-8a18-25287749b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "with shelve.open('./shelve/Phi-3.5-mini-answer') as db:\n",
    "    phi_35_mini_answers = db['processed']\n",
    "    phi_35_mini_no_answers  = db['remaining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24cc883-9b5d-4b7c-9a02-05bbc27dc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_dicts(phi_35_mini_answers).write_csv(\"./dataset/Phi-3.5-mini-answer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb458722-c9c8-418e-8b1f-670ae57b9aca",
   "metadata": {},
   "source": [
    "### RAG - neuralmagic/Phi-3-medium-128k-instruct-quantized.w8a8\n",
    "\n",
    "The model is selected based on what is running on `vllm-serve` in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b65761-c939-4d75-8d1a-0134bc5d227c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a3ab4-aa2a-4a4d-b7bb-3682e9fa3e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80097d2-b46a-43c9-b9b7-b962f7876b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf7a191f-78bd-4680-9625-7f173f269d8d",
   "metadata": {},
   "source": [
    "# Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf64d8-7666-4c92-b129-7445d757772f",
   "metadata": {},
   "source": [
    "# JUDGE NEED REFACTOR TO USE ASYNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e847ae-2a08-48ec-820c-adc8b5359ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_honor = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd7d95-09ee-4847-90f5-f2c50bf20e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def judge(question, answer, mode=\"retrieval\"):\n",
    "    if mode==\"retrieval\":\n",
    "        system_msg = \"\"\"\n",
    "You are an expert evaluator for a Vector Database retrieval that answer a recipe based on the user question.\n",
    "Your task is to analyze the relevance of the retrieved answer to the given question.\n",
    "Based on the relevance of the retrieved answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Definitions:\n",
    "- NON_RELEVANT: The answer does not relate to the user’s question.\n",
    "- PARTLY_RELEVANT: The answer addresses some aspects but omits or misinterprets key parts of the question.\n",
    "- RELEVANT: The answer fully addresses the question with correct, useful information.\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}\n",
    "\"\"\".strip()\n",
    "\n",
    "    elif mode==\"rag\":\n",
    "        system_msg = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system that answer creative and detailed cooking suggestions for a single recipe idea, and instructions.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Definitions:\n",
    "- NON_RELEVANT: The answer does not relate to the user’s question.\n",
    "- PARTLY_RELEVANT: The answer addresses some aspects but omits or misinterprets key parts of the question.\n",
    "- RELEVANT: The answer fully addresses the question with correct, useful information.\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}\n",
    "\"\"\".strip()\n",
    "    else:\n",
    "        raise Exception(\"Wrong mode\")\n",
    "\n",
    "    user_msg = f\"\"\"\n",
    "Here is the data for evaluation:\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{answer}\n",
    "\"\"\".strip()\n",
    "\n",
    "    verdict = await your_honor.chat(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg}\n",
    "        ],\n",
    "    )\n",
    "   \n",
    "    for _ in range(2):\n",
    "        try:\n",
    "            return json.loads(verdict)\n",
    "        except JSONDecodeError:\n",
    "            content = f\"{{{verdict}}}\"\n",
    "    raise JSONDecodeError(\"Failed to decode JSON after retry\", data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb1279-b99a-4dcf-ab4f-82f44e083850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_verdict(ans, mode=\"retrieval\"):\n",
    "    try:\n",
    "        q = ans['question']\n",
    "        a = ans['answer']\n",
    "        v = asyncio.run(judge(q, a, mode))\n",
    "        ans[\"relevance\"] = v[\"Relevance\"]\n",
    "        ans[\"explanation\"] = v[\"Explanation\"]\n",
    "        return ans\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f330be3f-130f-4bf9-8c3b-07b5a868354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_question = {\n",
    "    \"question\": \"How do I make a lemon herb baked salmon?\",\n",
    "    \"answer\": \"Title: Easy Herbed Grilled Salmon\\nIngredients:\\n1/2 lb. salmon filet\\n1 Tbsp. butter or margarine\\n1/2 lemon\\n2 Tbsp. white wine\\n1/2 tsp. salt (optional)\\n1/2 tsp. onion powder\\n1/2 tsp. garlic powder\\n1/2 tsp. lemon pepper\\n1 tsp. oregano\\n1/2 tsp. dill weed\\n1/2 tsp. parsley flakes\\n1/4 tsp. paprika\\nDirections:\\nPreheat grill. Make a tray out of heavy-duty foil by folding a long piece in half and folding up all 4 sides with the dull side up. Spray the bottom of the foil tray with cooking spray. Place fish filet in the tray, skin side down. Smear a thin line of butter on filet. Squeeze lemon juice liberally over filet and then a splash of white wine. Sprinkle remaining seasonings lightly over filet and transfer the foil tray to the hot grill. Cover. Cook for 10 minutes per inch of thickness of filet. DO NOT overcook or it will be dry and unpalatable. Turning is not necessary. Salmon is done when it turns a light pink color throughout.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529177a1-b57a-4edb-9ed0-09c0d7b0b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_verdict(mock_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2708e177-d4c0-4751-be69-0c27a8fbf9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_veredicts(answers, db_name, mode=\"retrieval\", num_threads=4, batch_size=250, attempt=1, max_attempts=3):\n",
    "    counter = 0\n",
    "\n",
    "    with shelve.open(f'./shelve/{db_name}') as db:\n",
    "        if 'remaining' not in db:\n",
    "            db['processed'] = []\n",
    "            db['remaining'] = answers\n",
    "\n",
    "        processed_answers = db['processed']\n",
    "        remaining_answers = db['remaining']\n",
    "\n",
    "    if mode==\"retrieval\":\n",
    "        remaining_answers_dict = {f\"{ans['question_id']}_{ans['rank']}\": ans for ans in remaining_answers}\n",
    "    else:\n",
    "        remaining_answers_dict = {str(ans['question_id']): ans for ans in remaining_answers}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(process_single_verdict, ans, mode) for ans in remaining_answers]\n",
    "\n",
    "        for future in tqdm(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                processed_answers.append(result)\n",
    "\n",
    "                if mode==\"retrieval\":\n",
    "                    remaining_answers_dict.pop(f\"{result['question_id']}_{result['rank']}\", None)\n",
    "                else:\n",
    "                    remaining_answers_dict.pop(str(result['question_id']), None)\n",
    "\n",
    "                counter += 1\n",
    "                if counter % batch_size == 0:\n",
    "                    remaining_answers = list(remaining_answers_dict.values())\n",
    "                    with shelve.open(f'./shelve/{db_name}') as db:\n",
    "                        db['processed'] = processed_answers\n",
    "                        db['remaining'] = remaining_answers\n",
    "\n",
    "    remaining_answers = list(remaining_answers_dict.values())\n",
    "    with shelve.open(f'./shelve/{db_name}') as db:\n",
    "        db['processed'] = processed_answers\n",
    "        db['remaining'] = remaining_answers\n",
    "\n",
    "    if remaining_answers and attempt <= max_attempts:\n",
    "        print(f\"Remaining answers: {len(remaining_answers)} | Attempt num: {attempt}\")\n",
    "        generate_veredicts(remaining_answers, db_name, mode, num_threads=num_threads, attempt=attempt+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f1bea8-33d8-4066-91e6-d3e87256c512",
   "metadata": {},
   "source": [
    "## Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c2eb3-0666-43b3-812a-3de0e0df7a76",
   "metadata": {},
   "source": [
    "#### Get answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd99885-d23a-493a-987d-c1dcb238f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_answers = pl.read_csv(\"./dataset/vector-answers.csv\").to_dicts()\n",
    "hybrid_answers = pl.read_csv(\"./dataset/hybrid-answers.csv\").to_dicts()\n",
    "qrewrite_answers = pl.read_csv(\"./dataset/qrewrite-answers.csv\").to_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af823f14-e314-409e-8ea7-2af65a83fb5c",
   "metadata": {},
   "source": [
    "### VectorSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0e64b-501d-4d53-aa99-a10894810392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_veredicts(vector_answers, 'vector_answers') #1:50:23 running in serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d4105-447e-4772-b9e4-2f21e9df89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with shelve.open('./shelve/vector_answers') as db:\n",
    "    vector_veredict = db['processed']\n",
    "    vector_no_veredict  = db['remaining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ebb4da-807f-43ab-aca7-89f197d1e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_dicts(vector_veredict).write_csv(\"./dataset/vector-veredict.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaca519-5c7a-46ce-82d4-6f238fdc9170",
   "metadata": {},
   "source": [
    "### HybridSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e4ac7-48a5-4052-b2cf-ca0e28899e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_veredicts(hybrid_answers, 'hybrid_answers') # 34:58 running in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417489d-6156-46e6-a4a0-dbb19fdbcfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with shelve.open('./shelve/hybrid_answers') as db:\n",
    "    hybrid_veredict = db['processed']\n",
    "    hybrid_no_veredict  = db['remaining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec9029-d49f-4ca7-ae2d-d530074159ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_dicts(hybrid_veredict).write_csv(\"./dataset/hybrid-veredict.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd467c-9ae8-45e1-8258-e0cc10897e94",
   "metadata": {},
   "source": [
    "### Query rewrite + HybridSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745026e-ed24-42ce-b27a-3693c96662b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_veredicts(qrewrite_answers, 'qrewrite_answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf699da-7057-4654-a8f6-1f0d9e26cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with shelve.open('./shelve/qrewrite_answers') as db:\n",
    "    qrewrite_veredict = db['processed']\n",
    "    qrewrite_no_veredict  = db['remaining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4690d1-2188-4325-9232-0eaa6ce07cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_dicts(qrewrite_veredict).write_csv(\"./dataset/qrewrite-veredict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de3037-e6fa-4022-920e-7df4fc747131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a94749-8ebb-4bcd-ab64-c5a46ac2e185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
